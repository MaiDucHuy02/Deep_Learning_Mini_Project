{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14311902,"sourceType":"datasetVersion","datasetId":9136482},{"sourceId":14342260,"sourceType":"datasetVersion","datasetId":9157450}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport torchvision\nfrom torchvision import datasets, models, transforms\nimport os\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport torchvision.models as models","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-31T09:02:16.329173Z","iopub.execute_input":"2025-12-31T09:02:16.329380Z","iopub.status.idle":"2025-12-31T09:02:24.403699Z","shell.execute_reply.started":"2025-12-31T09:02:16.329358Z","shell.execute_reply":"2025-12-31T09:02:24.402821Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"DATA_DIR = \"/kaggle/input/face-recognition-mini-project\"\nCATEGORIES = [  \"Brad Pitt\",\n                \"Charles Leclerc\",\n                \"Conor McGregor\",\n                \"David Beckham\",\n                \"Erling Haaland\",\n                \"Faker\",\n                \"General Vo Nguyen Giap\",\n                \"Huy\",\n                \"J97\",\n                \"Jeff Bezo\",\n                \"Jeffray\",\n                \"Joji\",\n                \"Khabib\",\n                \"Leonardo DiCaprio\",\n                \"Levi\",\n                \"Messi\",\n                \"Mixigaming\",\n                \"Park Hang-seo\",\n                \"Robert Downey Junior\",\n                \"Ronaldo\",\n                \"SonTungMTP\",\n                \"Taylor Swift\",\n                \"Tobey Maguire\",\n                \"Tom Hanks\",\n                \"Tom_Cruise\",\n                \"Will Smith\",\n                \"Zhao Lusi\",\n                \"antony\",\n                \"thayongnoi\"]\nIMG_SIZE = 300\nBATCH_SIZE = 32\nSEED = 42\n\ndevice = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\nprint(f\"Using {device} device\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T09:02:24.405150Z","iopub.execute_input":"2025-12-31T09:02:24.405500Z","iopub.status.idle":"2025-12-31T09:02:24.465464Z","shell.execute_reply.started":"2025-12-31T09:02:24.405474Z","shell.execute_reply":"2025-12-31T09:02:24.464861Z"}},"outputs":[{"name":"stdout","text":"Using cuda device\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),  # đưa ảnh về [0, 1]\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T09:02:24.466262Z","iopub.execute_input":"2025-12-31T09:02:24.466600Z","iopub.status.idle":"2025-12-31T09:02:24.486340Z","shell.execute_reply.started":"2025-12-31T09:02:24.466569Z","shell.execute_reply":"2025-12-31T09:02:24.485649Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class FaceRecognitionMiniProject(Dataset):\n    def __init__(self, data_dir, categories, transform=None):\n        self.image_paths = []\n        self.labels = []\n        self.transform = transform\n\n        for label, category in enumerate(categories):\n            class_path = os.path.join(data_dir, category)\n            for img_name in os.listdir(class_path):\n                img_path = os.path.join(class_path, img_name)\n                self.image_paths.append(img_path)\n                self.labels.append(label)\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n        label = self.labels[idx]\n\n        if self.transform:\n            image = self.transform(image)\n            \n        return image, label\n\nfull_dataset = FaceRecognitionMiniProject(\n    data_dir=DATA_DIR,\n    categories=CATEGORIES,\n    transform=transform\n)\n\nprint(f\"Loaded {len(full_dataset)} images.\")\n\n\ntotal_size = len(full_dataset)\ntrain_size = int(0.7 * total_size)\nval_size   = int(0.3 * total_size)\ntest_size  = total_size - train_size - val_size\n\ntrain_dataset, val_dataset, test_dataset = random_split(\n    full_dataset,\n    [train_size, val_size, test_size],\n    generator=torch.Generator().manual_seed(SEED)\n)\n\nprint(f\"Train: {len(train_dataset)}\")\nprint(f\"Validation: {len(val_dataset)}\")\nprint(f\"Test: {len(test_dataset)}\")\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\ntest_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n\nimages, labels = next(iter(train_loader))\nprint(\"Image batch shape:\", images.shape)   # [B, 3, 224, 224]\nprint(\"Label batch shape:\", labels.shape)   # [B]\nprint(\"Sample labels:\", labels[:5])\n\nnum_classes = len(CATEGORIES)\nprint(\"Number of classes:\", num_classes)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T09:02:24.487336Z","iopub.execute_input":"2025-12-31T09:02:24.487754Z","iopub.status.idle":"2025-12-31T09:02:25.219473Z","shell.execute_reply.started":"2025-12-31T09:02:24.487720Z","shell.execute_reply":"2025-12-31T09:02:25.218746Z"}},"outputs":[{"name":"stdout","text":"Loaded 600 images.\nTrain: 420\nValidation: 180\nTest: 0\nImage batch shape: torch.Size([32, 3, 300, 300])\nLabel batch shape: torch.Size([32])\nSample labels: tensor([ 6, 26,  2, 13,  9])\nNumber of classes: 29\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"train_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomRotation(degrees=15),\n    transforms.RandomResizedCrop(227, scale=(0.9, 1.0)),\n    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n\nval_test_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T09:02:25.220438Z","iopub.execute_input":"2025-12-31T09:02:25.220743Z","iopub.status.idle":"2025-12-31T09:02:25.225893Z","shell.execute_reply.started":"2025-12-31T09:02:25.220712Z","shell.execute_reply":"2025-12-31T09:02:25.225352Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def get_model():\n    weights = models.MobileNet_V2_Weights.IMAGENET1K_V2\n    model = models.mobilenet_v2(weights=weights)\n    for param in model.parameters():\n        param.requires_grad = False\n    model.classifier = nn.Sequential(\n    nn.Flatten(),\n    nn.Linear(1280, 256),\n    nn.ReLU(),\n    nn.Dropout(0.2),\n    nn.Linear(256, num_classes),\n    nn.Sigmoid())\n    loss_fn = loss_fn = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr= 1e-3)\n    return model.to(device), loss_fn, optimizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T09:02:25.226851Z","iopub.execute_input":"2025-12-31T09:02:25.227095Z","iopub.status.idle":"2025-12-31T09:02:25.243404Z","shell.execute_reply.started":"2025-12-31T09:02:25.227076Z","shell.execute_reply":"2025-12-31T09:02:25.242639Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"!pip install torch_summary\nfrom torchsummary import summary\nmodel, criterion, optimizer = get_model()\nsummary(model, torch.zeros(1,3,224,224))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T09:02:25.245535Z","iopub.execute_input":"2025-12-31T09:02:25.245829Z","iopub.status.idle":"2025-12-31T09:02:30.645144Z","shell.execute_reply.started":"2025-12-31T09:02:25.245806Z","shell.execute_reply":"2025-12-31T09:02:30.644494Z"}},"outputs":[{"name":"stdout","text":"Collecting torch_summary\n  Downloading torch_summary-1.4.5-py3-none-any.whl.metadata (18 kB)\nDownloading torch_summary-1.4.5-py3-none-any.whl (16 kB)\nInstalling collected packages: torch_summary\nSuccessfully installed torch_summary-1.4.5\nDownloading: \"https://download.pytorch.org/models/mobilenet_v2-7ebf99e0.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-7ebf99e0.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13.6M/13.6M [00:00<00:00, 192MB/s]\n","output_type":"stream"},{"name":"stdout","text":"====================================================================================================\nLayer (type:depth-idx)                             Output Shape              Param #\n====================================================================================================\n├─Sequential: 1-1                                  [-1, 1280, 7, 7]          --\n|    └─Conv2dNormActivation: 2-1                   [-1, 32, 112, 112]        --\n|    |    └─Conv2d: 3-1                            [-1, 32, 112, 112]        (864)\n|    |    └─BatchNorm2d: 3-2                       [-1, 32, 112, 112]        (64)\n|    |    └─ReLU6: 3-3                             [-1, 32, 112, 112]        --\n|    └─InvertedResidual: 2-2                       [-1, 16, 112, 112]        --\n|    |    └─Sequential: 3-4                        [-1, 16, 112, 112]        (896)\n|    └─InvertedResidual: 2-3                       [-1, 24, 56, 56]          --\n|    |    └─Sequential: 3-5                        [-1, 24, 56, 56]          (5,136)\n|    └─InvertedResidual: 2-4                       [-1, 24, 56, 56]          --\n|    |    └─Sequential: 3-6                        [-1, 24, 56, 56]          (8,832)\n|    └─InvertedResidual: 2-5                       [-1, 32, 28, 28]          --\n|    |    └─Sequential: 3-7                        [-1, 32, 28, 28]          (10,000)\n|    └─InvertedResidual: 2-6                       [-1, 32, 28, 28]          --\n|    |    └─Sequential: 3-8                        [-1, 32, 28, 28]          (14,848)\n|    └─InvertedResidual: 2-7                       [-1, 32, 28, 28]          --\n|    |    └─Sequential: 3-9                        [-1, 32, 28, 28]          (14,848)\n|    └─InvertedResidual: 2-8                       [-1, 64, 14, 14]          --\n|    |    └─Sequential: 3-10                       [-1, 64, 14, 14]          (21,056)\n|    └─InvertedResidual: 2-9                       [-1, 64, 14, 14]          --\n|    |    └─Sequential: 3-11                       [-1, 64, 14, 14]          (54,272)\n|    └─InvertedResidual: 2-10                      [-1, 64, 14, 14]          --\n|    |    └─Sequential: 3-12                       [-1, 64, 14, 14]          (54,272)\n|    └─InvertedResidual: 2-11                      [-1, 64, 14, 14]          --\n|    |    └─Sequential: 3-13                       [-1, 64, 14, 14]          (54,272)\n|    └─InvertedResidual: 2-12                      [-1, 96, 14, 14]          --\n|    |    └─Sequential: 3-14                       [-1, 96, 14, 14]          (66,624)\n|    └─InvertedResidual: 2-13                      [-1, 96, 14, 14]          --\n|    |    └─Sequential: 3-15                       [-1, 96, 14, 14]          (118,272)\n|    └─InvertedResidual: 2-14                      [-1, 96, 14, 14]          --\n|    |    └─Sequential: 3-16                       [-1, 96, 14, 14]          (118,272)\n|    └─InvertedResidual: 2-15                      [-1, 160, 7, 7]           --\n|    |    └─Sequential: 3-17                       [-1, 160, 7, 7]           (155,264)\n|    └─InvertedResidual: 2-16                      [-1, 160, 7, 7]           --\n|    |    └─Sequential: 3-18                       [-1, 160, 7, 7]           (320,000)\n|    └─InvertedResidual: 2-17                      [-1, 160, 7, 7]           --\n|    |    └─Sequential: 3-19                       [-1, 160, 7, 7]           (320,000)\n|    └─InvertedResidual: 2-18                      [-1, 320, 7, 7]           --\n|    |    └─Sequential: 3-20                       [-1, 320, 7, 7]           (473,920)\n|    └─Conv2dNormActivation: 2-19                  [-1, 1280, 7, 7]          --\n|    |    └─Conv2d: 3-21                           [-1, 1280, 7, 7]          (409,600)\n|    |    └─BatchNorm2d: 3-22                      [-1, 1280, 7, 7]          (2,560)\n|    |    └─ReLU6: 3-23                            [-1, 1280, 7, 7]          --\n├─Sequential: 1-2                                  [-1, 29]                  --\n|    └─Flatten: 2-20                               [-1, 1280]                --\n|    └─Linear: 2-21                                [-1, 256]                 327,936\n|    └─ReLU: 2-22                                  [-1, 256]                 --\n|    └─Dropout: 2-23                               [-1, 256]                 --\n|    └─Linear: 2-24                                [-1, 29]                  7,453\n|    └─Sigmoid: 2-25                               [-1, 29]                  --\n====================================================================================================\nTotal params: 2,559,261\nTrainable params: 335,389\nNon-trainable params: 2,223,872\nTotal mult-adds (M): 156.73\n====================================================================================================\nInput size (MB): 0.57\nForward/backward pass size (MB): 15.82\nParams size (MB): 9.76\nEstimated Total Size (MB): 26.15\n====================================================================================================\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"====================================================================================================\nLayer (type:depth-idx)                             Output Shape              Param #\n====================================================================================================\n├─Sequential: 1-1                                  [-1, 1280, 7, 7]          --\n|    └─Conv2dNormActivation: 2-1                   [-1, 32, 112, 112]        --\n|    |    └─Conv2d: 3-1                            [-1, 32, 112, 112]        (864)\n|    |    └─BatchNorm2d: 3-2                       [-1, 32, 112, 112]        (64)\n|    |    └─ReLU6: 3-3                             [-1, 32, 112, 112]        --\n|    └─InvertedResidual: 2-2                       [-1, 16, 112, 112]        --\n|    |    └─Sequential: 3-4                        [-1, 16, 112, 112]        (896)\n|    └─InvertedResidual: 2-3                       [-1, 24, 56, 56]          --\n|    |    └─Sequential: 3-5                        [-1, 24, 56, 56]          (5,136)\n|    └─InvertedResidual: 2-4                       [-1, 24, 56, 56]          --\n|    |    └─Sequential: 3-6                        [-1, 24, 56, 56]          (8,832)\n|    └─InvertedResidual: 2-5                       [-1, 32, 28, 28]          --\n|    |    └─Sequential: 3-7                        [-1, 32, 28, 28]          (10,000)\n|    └─InvertedResidual: 2-6                       [-1, 32, 28, 28]          --\n|    |    └─Sequential: 3-8                        [-1, 32, 28, 28]          (14,848)\n|    └─InvertedResidual: 2-7                       [-1, 32, 28, 28]          --\n|    |    └─Sequential: 3-9                        [-1, 32, 28, 28]          (14,848)\n|    └─InvertedResidual: 2-8                       [-1, 64, 14, 14]          --\n|    |    └─Sequential: 3-10                       [-1, 64, 14, 14]          (21,056)\n|    └─InvertedResidual: 2-9                       [-1, 64, 14, 14]          --\n|    |    └─Sequential: 3-11                       [-1, 64, 14, 14]          (54,272)\n|    └─InvertedResidual: 2-10                      [-1, 64, 14, 14]          --\n|    |    └─Sequential: 3-12                       [-1, 64, 14, 14]          (54,272)\n|    └─InvertedResidual: 2-11                      [-1, 64, 14, 14]          --\n|    |    └─Sequential: 3-13                       [-1, 64, 14, 14]          (54,272)\n|    └─InvertedResidual: 2-12                      [-1, 96, 14, 14]          --\n|    |    └─Sequential: 3-14                       [-1, 96, 14, 14]          (66,624)\n|    └─InvertedResidual: 2-13                      [-1, 96, 14, 14]          --\n|    |    └─Sequential: 3-15                       [-1, 96, 14, 14]          (118,272)\n|    └─InvertedResidual: 2-14                      [-1, 96, 14, 14]          --\n|    |    └─Sequential: 3-16                       [-1, 96, 14, 14]          (118,272)\n|    └─InvertedResidual: 2-15                      [-1, 160, 7, 7]           --\n|    |    └─Sequential: 3-17                       [-1, 160, 7, 7]           (155,264)\n|    └─InvertedResidual: 2-16                      [-1, 160, 7, 7]           --\n|    |    └─Sequential: 3-18                       [-1, 160, 7, 7]           (320,000)\n|    └─InvertedResidual: 2-17                      [-1, 160, 7, 7]           --\n|    |    └─Sequential: 3-19                       [-1, 160, 7, 7]           (320,000)\n|    └─InvertedResidual: 2-18                      [-1, 320, 7, 7]           --\n|    |    └─Sequential: 3-20                       [-1, 320, 7, 7]           (473,920)\n|    └─Conv2dNormActivation: 2-19                  [-1, 1280, 7, 7]          --\n|    |    └─Conv2d: 3-21                           [-1, 1280, 7, 7]          (409,600)\n|    |    └─BatchNorm2d: 3-22                      [-1, 1280, 7, 7]          (2,560)\n|    |    └─ReLU6: 3-23                            [-1, 1280, 7, 7]          --\n├─Sequential: 1-2                                  [-1, 29]                  --\n|    └─Flatten: 2-20                               [-1, 1280]                --\n|    └─Linear: 2-21                                [-1, 256]                 327,936\n|    └─ReLU: 2-22                                  [-1, 256]                 --\n|    └─Dropout: 2-23                               [-1, 256]                 --\n|    └─Linear: 2-24                                [-1, 29]                  7,453\n|    └─Sigmoid: 2-25                               [-1, 29]                  --\n====================================================================================================\nTotal params: 2,559,261\nTrainable params: 335,389\nNon-trainable params: 2,223,872\nTotal mult-adds (M): 156.73\n====================================================================================================\nInput size (MB): 0.57\nForward/backward pass size (MB): 15.82\nParams size (MB): 9.76\nEstimated Total Size (MB): 26.15\n===================================================================================================="},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"num_epochs = 30\n\ntrain_acc_list = []\nval_acc_list = []\nbest_val_acc = 0.0\nbest_model_path = 'best_inception_model.pth'\n\n\nfor epoch in range(num_epochs):\n    # ===== TRAIN =====\n    model.train()\n    correct, total = 0, 0\n\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    train_acc = 100 * correct / total\n    train_acc_list.append(train_acc)\n\n    # ===== VALIDATION =====\n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    val_acc = 100 * correct / total\n    val_acc_list.append(val_acc)\n\n    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n          f\"Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), 'best_model_weights.pth')\n        print(f\"✓ Đã lưu model tốt nhất với Val Acc: {val_acc:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T09:02:30.646374Z","iopub.execute_input":"2025-12-31T09:02:30.646668Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/30] Train Acc: 9.76% | Val Acc: 6.11%\n✓ Đã lưu model tốt nhất với Val Acc: 6.11%\nEpoch [2/30] Train Acc: 11.67% | Val Acc: 12.22%\n✓ Đã lưu model tốt nhất với Val Acc: 12.22%\nEpoch [3/30] Train Acc: 34.05% | Val Acc: 38.89%\n✓ Đã lưu model tốt nhất với Val Acc: 38.89%\nEpoch [4/30] Train Acc: 52.62% | Val Acc: 42.78%\n✓ Đã lưu model tốt nhất với Val Acc: 42.78%\nEpoch [5/30] Train Acc: 56.90% | Val Acc: 47.78%\n✓ Đã lưu model tốt nhất với Val Acc: 47.78%\nEpoch [6/30] Train Acc: 70.48% | Val Acc: 47.22%\nEpoch [7/30] Train Acc: 66.90% | Val Acc: 45.56%\nEpoch [8/30] Train Acc: 78.10% | Val Acc: 55.56%\n✓ Đã lưu model tốt nhất với Val Acc: 55.56%\nEpoch [9/30] Train Acc: 76.67% | Val Acc: 51.11%\nEpoch [10/30] Train Acc: 81.67% | Val Acc: 56.11%\n✓ Đã lưu model tốt nhất với Val Acc: 56.11%\nEpoch [11/30] Train Acc: 87.38% | Val Acc: 59.44%\n✓ Đã lưu model tốt nhất với Val Acc: 59.44%\nEpoch [12/30] Train Acc: 88.57% | Val Acc: 56.67%\nEpoch [13/30] Train Acc: 84.76% | Val Acc: 57.78%\nEpoch [14/30] Train Acc: 91.19% | Val Acc: 57.78%\nEpoch [15/30] Train Acc: 90.71% | Val Acc: 65.56%\n✓ Đã lưu model tốt nhất với Val Acc: 65.56%\nEpoch [16/30] Train Acc: 93.10% | Val Acc: 55.56%\nEpoch [17/30] Train Acc: 91.90% | Val Acc: 59.44%\nEpoch [18/30] Train Acc: 91.67% | Val Acc: 61.67%\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(8, 5))\nplt.plot(train_acc_list, label=\"Train Accuracy\")\nplt.plot(val_acc_list, label=\"Validation Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy (%)\")\nplt.title(\"Training Accuracy\")\nplt.legend()\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}